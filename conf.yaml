# Set paths here 
pretrained_model_name_or_path: null # Path to pretrained model or model identifier from huggingface.co/models
controlnet_model_name_or_path: null # Path to pretrained controlnet model or model identifier from huggingface.co/models. If not specified controlnet weights are initialized from unet.
revision: null # Revision of pretrained model identifier from huggingface.co/models.
varaint: null # Variant of the model files of the pretrained model identifier from huggingface.co/models, 'e.g.' fp16
tokenizer_name: null # Pretrained tokenizer name or path if not the same as model_name
output_dir: "controlnet-model" # The output directory where the model predictions and checkpoints will be written.
cache_dir: null # The directory where the downloaded models and datasets will be stored.

# traning settings
seed: null # A seed for reproducible training.
resolution: 512 # The resolution for input images, all the images in the train/validation dataset will be resized to this resolution.
train_batch_size: 4 # Batch size (per device) for the training dataloader.
num_train_epochs: 1 #
max_train_steps: null # Total number of training steps to perform. If provided, overrides num_train_epochs.
checkpointing_steps: 500 # Save a checkpoint of the training state every X updates. Checkpoints can be used for resuming training via `--resume_from_checkpoint`.
checkpoints_total_limit: null # Max number of checkpoints to store.
resume_from_checkpoint: null # hether training should be resumed from a previous checkpoint. Use a path saved by `--checkpointing_steps`, or `"latest"` to automatically select the last available checkpoint.


# lr and optimization
learning_rate: 0.000005 # Initial learning rate (after the potential warmup period) to use.
lr_scheduler: "constant" # The scheduler type to use. Choose between ["linear", "cosine", "cosine_with_restarts", "polynomial", "constant", "constant_with_warmup"
lr_warmup_steps: 500 # Number of steps for the warmup in the lr scheduler.
lr_num_cycles: 1 # Number of hard resets of the lr in cosine_with_restarts scheduler.
lr_power: 1.0 # Power factor of the polynomial scheduler.
dataloader_num_workers: 0 # Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process.
adam_beta1: 0.9 # The beta1 parameter for the Adam optimizer.
adam_beta2: 0.999 # The beta2 parameter for the Adam optimizer.
adam_weight_decay: 0.01 # Weight decay to use.
adam_epsilon: 0.00000001 # Epsilon value for the Adam optimizer
max_grad_norm: 1.0 # Max gradient norm.

# accelerate optimization
allow_tf32: False # Whether or not to allow TF32 on Ampere GPUs. Can be used to speed up training. 
mixed_precision: null # ["no", "fp16", "bf16"]
enable_xformers_memory_efficient_attention: False # Whether or not to use xformers.
use_8bit_adam: False # Whether or not to use 8-bit Adam from bitsandbytes.
set_grads_to_none: False # Save more memory by using setting grads to None instead of zero. 
gradient_accumulation_steps: 1 # "Number of updates steps to accumulate before performing a backward/update pass.
gradient_checkpointing: False # Whether or not to use gradient checkpointing to save memory at the expense of slower backward pass.

# logging
hub_model_id: null # The name of the repository to keep in sync with the local `output_dir`.
logging_dir: "logs/" # [TensorBoard](https://www.tensorflow.org/tensorboard) log directory. Will default to  *output_dir/runs/**CURRENT_DATETIME_HOSTNAME***.
report_to: "tensorboard" # The integration to report the results and logs to. Supported platforms are `"tensorboard" (default), `"wandb"` and `"comet_ml"`. Use `"all"` to report to all integrations.

# datasets and samples
dataset_name: null # The name of the Dataset (from the HuggingFace hub) to train on.
dataset_config_name: null # The config of the Dataset, leave as None if there's only one config.
train_data_dir: null # A folder containing the training data. 
image_column: "image" # The column of the dataset containing the target image.
conditioning_image_column: "conditioning_image" # The column of the dataset containing the controlnet conditioning image.
caption_column: "text" # The column of the dataset containing a caption or a list of captions.
max_train_samples: null # For debugging purposes or quicker training, truncate the number of training examples to this value if set.
proportion_empty_prompts: 0 # Proportion of image prompts to be replaced with empty strings. Defaults to 0 (no prompt replacement)
validation_prompt: null #  A set of prompts evaluated every `--validation_steps`
validation_image: null # A set of paths to the controlnet conditioning image be evaluated every `--validation_steps`
num_validation_images: 4 # Number of images to be generated for each `--validation_image`
validation_steps: 100 # Run validation every X steps.
tracker_project_name: "train_controlnet" # "The `project_name` argument passed to Accelerator.init_trackers